{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a49f327-fa50-4a34-87df-364a9c81265f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/xiluo/miniconda3/envs/multiquant/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The model is not an instance of Wav2Vec2ForCTC. \"lm_head\" module is not imported.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ['TRANSFORMERS_CACHE'] = \"/jagupard28/scr0/xiluo-speech/multi-quantizer-experiments/hf/checkpoints/\"\n",
    "\n",
    "from transformers import AutoModel\n",
    "import torchaudio\n",
    "from torchaudio.models.wav2vec2.utils import import_huggingface_model\n",
    "\n",
    "hf_model = AutoModel.from_pretrained('facebook/hubert-large-ls960-ft').to('cuda') #TODO: change to hubert-large-ls960-ft\n",
    "assert hf_model.__class__.__name__ in {\"Wav2Vec2Model\", \"HubertModel\"}\n",
    "\n",
    "teacher_model = import_huggingface_model(hf_model).eval().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49c5b44-86ab-4055-a3da-f645ee404fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset parts:   0%|                                                                                         | 0/4 [00:00<?, ?it/s]\n",
      "Distributing tasks: 0it [00:00, ?it/s]\u001b[A\n",
      "                                      \u001b[A\n",
      "Processing:   0%|                                                                                         | 0/1519 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  40%|██████████████████████████████▊                                              | 608/1519 [00:00<00:00, 6076.37it/s]\u001b[A\n",
      "Processing:  87%|██████████████████████████████████████████████████████████████████▏         | 1323/1519 [00:00<00:00, 6704.91it/s]\u001b[A\n",
      "Dataset parts:  25%|████████████████████▎                                                            | 1/4 [00:00<00:01,  2.45it/s]\u001b[A\n",
      "Distributing tasks: 0it [00:00, ?it/s]\u001b[A\n",
      "Distributing tasks: 43it [00:00, 306.82it/s]\u001b[A\n",
      "Distributing tasks: 110it [00:00, 488.18it/s]\u001b[A\n",
      "Distributing tasks: 162it [00:00, 388.91it/s]\u001b[A\n",
      "Distributing tasks: 228it [00:00, 473.63it/s]\u001b[A\n",
      "Distributing tasks: 280it [00:00, 396.90it/s]\u001b[A\n",
      "Distributing tasks: 346it [00:00, 465.68it/s]\u001b[A\n",
      "Distributing tasks: 412it [00:00, 518.04it/s]\u001b[A\n",
      "Distributing tasks: 468it [00:01, 422.10it/s]\u001b[A\n",
      "Distributing tasks: 534it [00:01, 478.68it/s]\u001b[A\n",
      "                                             \u001b[A\n",
      "Processing:   0%|                                                                                        | 0/28539 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  11%|███████▉                                                                  | 3085/28539 [00:00<00:00, 30828.35it/s]\u001b[A\n",
      "Processing:  22%|████████████████▏                                                          | 6168/28539 [00:00<00:02, 8438.29it/s]\u001b[A\n",
      "Processing:  27%|████████████████████▍                                                      | 7767/28539 [00:00<00:02, 8065.06it/s]\u001b[A\n",
      "Processing:  31%|███████████████████████▌                                                   | 8970/28539 [00:01<00:02, 7861.93it/s]\u001b[A\n",
      "Processing:  35%|██████████████████████████▏                                                | 9987/28539 [00:01<00:02, 7683.98it/s]\u001b[A\n",
      "Processing:  38%|████████████████████████████▎                                             | 10895/28539 [00:01<00:02, 7551.14it/s]\u001b[A\n",
      "Processing:  41%|██████████████████████████████▍                                           | 11737/28539 [00:01<00:02, 7487.75it/s]\u001b[A\n",
      "Processing:  44%|████████████████████████████████▌                                         | 12542/28539 [00:01<00:02, 7361.66it/s]\u001b[A\n",
      "Processing:  47%|██████████████████████████████████▌                                       | 13313/28539 [00:01<00:02, 7303.46it/s]\u001b[A\n",
      "Processing:  49%|████████████████████████████████████▍                                     | 14066/28539 [00:01<00:02, 7041.34it/s]\u001b[A\n",
      "Processing:  52%|██████████████████████████████████████▎                                   | 14784/28539 [00:01<00:01, 7054.90it/s]\u001b[A\n",
      "Processing:  54%|████████████████████████████████████████▏                                 | 15508/28539 [00:01<00:01, 7102.73it/s]\u001b[A\n",
      "Processing:  57%|██████████████████████████████████████████                                | 16226/28539 [00:02<00:01, 7085.97it/s]\u001b[A\n",
      "Processing:  59%|███████████████████████████████████████████▉                              | 16947/28539 [00:02<00:01, 7119.29it/s]\u001b[A\n",
      "Processing:  62%|█████████████████████████████████████████████▊                            | 17663/28539 [00:02<00:01, 7091.28it/s]\u001b[A\n",
      "Processing:  64%|███████████████████████████████████████████████▋                          | 18375/28539 [00:02<00:01, 7086.76it/s]\u001b[A\n",
      "Processing:  67%|█████████████████████████████████████████████████▍                        | 19086/28539 [00:02<00:01, 7054.51it/s]\u001b[A\n",
      "Processing:  69%|███████████████████████████████████████████████████▎                      | 19793/28539 [00:02<00:01, 6991.13it/s]\u001b[A\n",
      "Processing:  72%|█████████████████████████████████████████████████████▏                    | 20493/28539 [00:02<00:01, 6895.91it/s]\u001b[A\n",
      "Processing:  74%|███████████████████████████████████████████████████████                   | 21214/28539 [00:02<00:01, 6987.41it/s]\u001b[A\n",
      "Processing:  77%|████████████████████████████████████████████████████████▊                 | 21919/28539 [00:02<00:00, 7003.94it/s]\u001b[A\n",
      "Processing:  79%|██████████████████████████████████████████████████████████▋               | 22625/28539 [00:02<00:00, 7020.17it/s]\u001b[A\n",
      "Processing:  82%|████████████████████████████████████████████████████████████▍             | 23328/28539 [00:03<00:00, 6938.31it/s]\u001b[A\n",
      "Processing:  84%|██████████████████████████████████████████████████████████████▎           | 24023/28539 [00:03<00:00, 4664.25it/s]\u001b[A\n",
      "Processing:  87%|████████████████████████████████████████████████████████████████▏         | 24754/28539 [00:03<00:00, 5247.36it/s]\u001b[A\n",
      "Processing:  89%|█████████████████████████████████████████████████████████████████▉        | 25412/28539 [00:03<00:00, 5565.94it/s]\u001b[A\n",
      "Processing:  91%|███████████████████████████████████████████████████████████████████▌      | 26073/28539 [00:03<00:00, 5831.07it/s]\u001b[A\n",
      "Processing:  94%|█████████████████████████████████████████████████████████████████████▌    | 26808/28539 [00:03<00:00, 6236.08it/s]\u001b[A\n",
      "Processing:  96%|███████████████████████████████████████████████████████████████████████▎  | 27524/28539 [00:03<00:00, 6489.43it/s]\u001b[A\n",
      "Processing:  99%|█████████████████████████████████████████████████████████████████████████▏| 28229/28539 [00:03<00:00, 6646.74it/s]\u001b[A\n",
      "Dataset parts:  50%|████████████████████████████████████████▌                                        | 2/4 [00:08<00:09,  4.79s/it]\u001b[A\n",
      "Distributing tasks: 0it [00:00, ?it/s]\u001b[A\n",
      "                                      \u001b[A\n",
      "Processing:   0%|                                                                                         | 0/2703 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  39%|█████████████████████████████▎                                             | 1057/2703 [00:00<00:00, 10563.24it/s]\u001b[A\n",
      "Processing:  78%|███████████████████████████████████████████████████████████▍                | 2114/2703 [00:00<00:00, 8074.82it/s]\u001b[A\n",
      "Dataset parts:  75%|████████████████████████████████████████████████████████████▊                    | 3/4 [00:08<00:02,  2.93s/it]\u001b[A\n",
      "Distributing tasks: 0it [00:00, ?it/s]\u001b[A\n",
      "                                      \u001b[A\n",
      "Processing:   0%|                                                                                         | 0/1089 [00:00<?, ?it/s]\u001b[A\n",
      "Processing:  82%|██████████████████████████████████████████████████████████████▊              | 888/1089 [00:00<00:00, 8868.53it/s]\u001b[A\n",
      "Dataset parts: 100%|█████████████████████████████████████████████████████████████████████████████████| 4/4 [00:09<00:00,  2.31s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "from lhotse.recipes import download_librispeech, prepare_librispeech\n",
    "\n",
    "# download_librispeech(dataset_parts=\"librispeech\")\n",
    "libri = prepare_librispeech(corpus_dir=\"./data/LibriSpeech/LibriSpeech\", output_dir=\"./data/LibriSpeech/manifests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3091197-1f60-48b0-8631-5d143caa1f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from lhotse import CutSet\n",
    "from lhotse.dataset import BucketingSampler\n",
    "from lhotse.dataset.input_strategies import AudioSamples\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class AudioSamplesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "      self.collator = AudioSamples()\n",
    "\n",
    "    def __getitem__(self, cuts: CutSet) -> dict:\n",
    "        audio_padded, audio_lengths = self.collator(cuts)\n",
    "        # print(cuts[0])\n",
    "        cut_ids = [cuts[i].supervisions[0].id for i in range(len(cuts))]\n",
    "        return { \"audio_padded\": audio_padded, \"audio_lengths\": audio_lengths, \"cut_ids\": cut_ids}\n",
    "\n",
    "cuts_train = CutSet.from_manifests(**libri[\"train-clean-100\"])\n",
    "\n",
    "train_sampler = BucketingSampler(\n",
    "    cuts_train,\n",
    "    max_duration=60,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    AudioSamplesDataset(),\n",
    "    sampler=train_sampler,\n",
    "    batch_size=None,\n",
    "    num_workers=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c8082-9983-4d57-a923-412440c9d486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "570f309e-5e57-4311-99ad-c66d8faa37da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio_padded': tensor([[ 5.1880e-04,  5.1880e-04,  4.8828e-04,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [-6.1035e-05,  0.0000e+00,  9.1553e-05,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 3.2349e-03,  4.2419e-03,  4.7302e-03,  ..., -3.9673e-04,\n",
       "          -6.4087e-04, -7.6294e-04]]),\n",
       " 'audio_lengths': tensor([242640, 237840, 242880], dtype=torch.int32),\n",
       " 'cut_ids': ['3879-174923-0028', '2289-152257-0023', '5463-39173-0008']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c98d55a6-c91b-4ee9-92db-47e69cae7350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 758, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which wav2vec 2/HuBERT transformer layer?\n",
    "LAYER_OF_INTEREST=24\n",
    "layer_index=LAYER_OF_INTEREST - 1\n",
    "\n",
    "activations = []\n",
    "\n",
    "# Register hook to trigger whenever forward() of nth layer is called\n",
    "teacher_model.encoder.transformer.layers[layer_index].register_forward_hook(\n",
    "    # Append outputs to list\n",
    "    lambda teacher_model, inputs, outputs: activations.append(outputs.detach())\n",
    "    # lambda teacher_model, inputs, outputs: print(outputs.shape)\n",
    "  )\n",
    "\n",
    "with torch.no_grad():\n",
    "    final_activations, feat_lens=teacher_model(batch['audio_padded'].to('cuda'), batch[\"audio_lengths\"].to('cuda'))\n",
    "int_activations=activations[0]\n",
    "activations.clear()\n",
    "int_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c077dee-5cb2-442d-9fb8-22037c9d67d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Quantizer(\n",
       "  (to_logits): Linear(in_features=1024, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multi_quantization import Quantizer\n",
    "import torch\n",
    "\n",
    "quantizer = Quantizer(\n",
    "            dim=int_activations.shape[-1],\n",
    "            num_codebooks=16,\n",
    "            codebook_size=256,\n",
    "        )\n",
    "\n",
    "quantizer.load_state_dict(torch.load('./quantizers/full-layer24-16N-quantizer.pt'))\n",
    "quantizer.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c641e6ee-10c3-4280-9f59-0c881f2f9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df6a237-f992-48be-8559-1f15a0e37125",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32d46338-cc9a-48f5-85b0-15e0ff5d2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuts_id_list = []\n",
    "codebook_indexes_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fb92956-284f-4523-b294-43df51fc9791",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/nlp/scr/xiluo/miniconda3/envs/multiquant/lib/python3.9/site-packages/multi_quantization/quantization.py:539: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  index=(this_indexes//saved_K).expand(*this_indexes.shape[:-1], dim)) +\n",
      "7432it [1:18:19,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(train_loader):\n",
    "    # embeddings = huber_model.extract_features(batch, layer=9)\n",
    "\n",
    "    # codebook_indexes = quantizer.encode(encoder_embedding)\n",
    "    # # [N, T, C]\n",
    "    # codebook_indexes = codebook_indexes.to(\"cpu\").numpy()\n",
    "    \n",
    "    # print(codebook_indexes)\n",
    "    \n",
    "    # for now save to csv\n",
    "    # |  cut.cut_id  | codebook_indices |\n",
    "    # | 123-456-123  | 31, 21, 30, 60   |\n",
    "    with torch.no_grad():\n",
    "        final_activations, feat_lens=teacher_model(batch['audio_padded'].to('cuda'), batch[\"audio_lengths\"].to('cuda'))\n",
    "    int_activations=activations[0]\n",
    "    activations.clear()\n",
    "    \n",
    "    codebook_indexes = quantizer.encode(int_activations)\n",
    "    # print(codebook_indexes.shape)\n",
    "    codebook_indexes = codebook_indexes.to(\"cpu\").numpy()\n",
    "    # print(codebook_indexes)\n",
    "\n",
    "    for i in range(codebook_indexes.shape[0]):\n",
    "        cuts_id_list.append(batch['cut_ids'][i])\n",
    "        codebook_indexes_list.append(codebook_indexes[i])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81939a3-8a2a-4808-91b7-03f30d70ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_df = pd.DataFrame({\n",
    "    'cuts_id' : cuts_id_list,\n",
    "    'codebook_indexes' : codebook_indexes_list\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5971f10e-b122-4403-9170-cd78cec6ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./codebook_indexes/codebook_indexes_layer24_N16.pickle', 'wb') as handle:\n",
    "    pickle.dump(feats_df, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f5af58-598b-43c0-b1ca-06662ce907f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./codebook_indexes/codebook_indexes_layer24_N16.pickle', 'rb') as handle:\n",
    "    pickle_example = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9112e31-0cc6-43d7-a230-5aabc4b6afa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81b573b-cbf5-4b6a-be38-ffbbde7059fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
